{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff687b9d-5db6-4978-bac8-47bd7d7b0003",
   "metadata": {},
   "source": [
    "# QUES NO 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6de71d-11a0-4633-8063-ee453ba294c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOSTING TECHNIQUE\n",
    "Boosting  is a ensemblimg   model technique in  which it tries to build the strong classifier from the \n",
    "number of weak classifier ,it is done by building a model by using weak model in series . In the first  step\n",
    "a model is built by training data. after that second model is build which tries to correct the error present \n",
    "in the first model .this procedure is continued and model are added until either the complete training data \n",
    "set is predicted correctly or the  maximum number are dded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb16a64-725f-4dcb-a614-c9a3be4b2081",
   "metadata": {},
   "source": [
    "# QUES NO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b921b-e430-4b69-9887-aa17d11d42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Boosting \n",
    "\n",
    "Improved Accuracy –\n",
    "\n",
    "Boosting can improve the accuracy of the model by combining several weak models’\n",
    "accuracies and averaging them for regression or voting over them for classification to increase the accuracy \n",
    "of the final model. \n",
    "\n",
    "Robustness to Overfitting – \n",
    "\n",
    "Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly. \n",
    "Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data\n",
    "points that are misclassified \n",
    "\n",
    "Better Interpretability –\n",
    "\n",
    "Boosting can increase the interpretability of the model by breaking the model decision process into\n",
    "multiple processes.  \n",
    "\n",
    "Limitation of boosting \n",
    "\n",
    "Boosting algorithms also have some disadvantages these are:\n",
    "\n",
    "Boosting Algorithms are vulnerable to the outliers \n",
    "\n",
    "It is difficult to use boosting algorithms for Real-Time applications.\n",
    "\n",
    " It is computationally expensive for large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50bf803-829d-44d3-86b7-49c227c00b1e",
   "metadata": {},
   "source": [
    "# QUES NO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5ac73-aab3-4e09-b626-f6360f60fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "For choosing the right distribution, here are the following steps:\n",
    "\n",
    "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation\n",
    "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher\n",
    "attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves\n",
    "the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have\n",
    "higher errors by preceding weak rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c28049-490a-4b21-bd05-e4c96c83103f",
   "metadata": {},
   "source": [
    "# QUES NO 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e16580-618a-4d9e-bafc-4481c09f61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Types Of Boosting Algorithms\n",
    "There are several types of boosting algorithms some of the most famous and useful models are as\n",
    "1:Ada boost \n",
    "2:Gradient boost \n",
    "3:Xgboost\n",
    "4:Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e26c9-7d39-405a-ab0a-6012e7c5d62a",
   "metadata": {},
   "source": [
    "# QUES NO 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4935b-8919-467e-b0b7-d2c03272d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "some common parameters in boosting algorithms\n",
    "1: Learning rate \n",
    "It is the technique  by which it shrinkage the value between 0  and 1\n",
    "2: maximum tree depth (max_depth)\n",
    "3: minimum child weight (min_child_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f8083-7235-4481-b6e3-cd243d153a31",
   "metadata": {},
   "source": [
    "# QUES NO 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba39d6-67ca-4fa1-8330-d2133924091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1\n",
    "The boosting algorithm assigns equal weight to each data sample. It feeds the data to the first machine\n",
    "model, called the base algorithm. The base algorithm makes predictions for each data sample.\n",
    "\n",
    "Step 2\n",
    "The boosting algorithm assesses model predictions and increases the weight of samples with a more significant\n",
    "error. It also assigns a weight based on model performance. A model that outputs excellent predictions will\n",
    "have a high amount of influence over the final decision.\n",
    "\n",
    "Step 3\n",
    "The algorithm passes the weighted data to the next decision tree.\n",
    "\n",
    "Step 4\n",
    "The algorithm repeats steps 2 and 3 until instances of training errors are below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea115b76-5943-4011-a0b6-34c120c3612b",
   "metadata": {},
   "source": [
    "# QUES NO  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48d92b-45ab-4d77-a440-3b6783a709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "First, let us discuss how boosting works. It makes ‘n’ number of decision trees during the data training\n",
    "period. As the first decision tree/model is made, the incorrectly classified record in the first model is \n",
    "given priority. Only these records are sent as input for the second model. The process goes on until we \n",
    "specify a number of base learners we want to create. Remember, repetition of records is allowed with all\n",
    "boosting techniques.\n",
    "\n",
    "This figure shows how the first model is made and errors from the first model are noted by the algorithm.\n",
    "The record which is incorrectly classified is used as input for the next model. This process is repeated\n",
    "until the specified condition is met. As you can see in the figure, there are ‘n’ number of models made by\n",
    "taking the errors from the previous model. This is how boosting works. The models 1,2, 3,…, N are individual\n",
    "models that can be known as decision trees. All types of boosting models work on the same principle. \n",
    "\n",
    "Since we now know the boosting principle, it will be easy to understand the AdaBoost algorithm. Let’s dive \n",
    "into AdaBoost’s working. When the random forest is used, the algorithm makes an ‘n’ number of trees. It makes\n",
    "proper trees that consist of a start node with several leaf nodes. Some trees might be bigger than others,\n",
    "but there is no fixed depth in a random forest. With AdaBoost, however, the algorithm only makes a node\n",
    "with two leaves, known as Stump.\n",
    "\n",
    "The figure here represents the stump. It can be seen clearly that it has only one node with two leaves.\n",
    "These stumps are weak learners and boosting techniques prefer this. The order of stumps is very important \n",
    "in AdaBoost. The error of the first stump influences how other stumps are made. Let’s understand this with\n",
    "an example. \n",
    "\n",
    "Here’s a sample dataset consisting of only three features where the output is in categorical form. The image\n",
    "shows the actual representation of the dataset. As the output is in binary/categorical form, it becomes a\n",
    "classification problem. In real life, the dataset can have any number of records and features in it. \n",
    "Let us consider 5 datasets for explanation purposes. The output is in categorical form, here in the form\n",
    "of Yes or No. All these records will be assigned a sample weight. The formula used for this is ‘W=1/N’ where\n",
    "N is the number of records. In this dataset, there are only 5 records, so the sample weight becomes 1/5\n",
    "initially. Every record gets the same weight. In this case, it’s 1/5. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e7d82-d2dc-4720-81c0-f0d58a816c83",
   "metadata": {},
   "source": [
    "# QUES NO 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a9d99-7f2e-48c6-94d0-7091cb57cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The error function that AdaBoost uses is an exponential loss function. First we find the products\n",
    "between the true values of training samples and the overall prediction for each sample. Then we take\n",
    "the sum of all the exponentials of these products in order to compute the error at iteration m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73460a25-744b-41bd-84ae-2ced7ddc098a",
   "metadata": {},
   "source": [
    "# QUES NO 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a37a55-0a2b-4407-a839-e2b165382c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initially all the data points have equal probability of getting selected, that is each data point has \n",
    "a weight equal to 1/N. In each iteration the weight of a data point gets changed in such a way, that it \n",
    "gets decreased, if it is correctly classified by the model generated in that iteration and increased \n",
    "otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90636ea-098e-42b2-9f21-e2e8efeed102",
   "metadata": {},
   "source": [
    "# QUES NO 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118bd88-6439-4434-8e55-5937307dbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    " This is the number of trees you want to build before taking the maximum voting or averages of predictions.\n",
    "    Higher number of trees give you better performance but makes your code slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba2eae-703b-4853-83dd-ce26613372db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5ee22-8229-4bc5-8b24-283cbbfb7b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
